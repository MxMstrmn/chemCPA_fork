{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training chemCPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has two dependencies:\n",
    "\n",
    "sciplex3\n",
    "wget \n",
    "rdkit embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a self-contained way to train the chemCPA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import wandb\n",
    "from hydra import compose, initialize\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from omegaconf import OmegaConf\n",
    "from plotnine import aes, geom_boxplot, ggplot, scale_y_continuous\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from chemCPA.data import DataModule\n",
    "from chemCPA.model import ComPert\n",
    "from chemCPA.train import evaluate_logfold_r2, evaluate_r2, evaluate_r2_sc\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load config\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../experiments/hydra_config\"):\n",
    "    config = compose(config_name=\"defaults\", overrides=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_path: /nfs/homedirs/hetzell/code/chemCPA_fork/project_folder/datasets/sciplex_complete_middle_subset_lincs_genes.h5ad\n",
      "drug_key: condition\n",
      "dose_key: dose\n",
      "knockout_key: null\n",
      "covariate_keys:\n",
      "- cell_type\n",
      "smiles_key: SMILES\n",
      "pert_category: cov_drug_dose_name\n",
      "split_key: split_ood_multi_task\n",
      "degs_key: lincs_DEGs\n",
      "drugs_embeddings: /nfs/homedirs/hetzell/code/chemCPA_fork/project_folder/embeddings/rdkit/data/embeddings/rdkit2D_embedding_lincs_trapnell.parquet\n",
      "knockouts_embeddings: null\n",
      "return_dataset: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.dataset.data_params))\n",
    "assert (Path(config.dataset.data_params.dataset_path)).exists(), \"Config `dataset_path` is not correct!\"\n",
    "assert (Path(config.dataset.data_params.drugs_embeddings)).exists(), \"Config `drugs_embeddings` is not correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check adata if required\n",
    "\n",
    "# # If you want to load the adata from the dataset_path\n",
    "# ADATA_PATH = Path(config.dataset.data_params.dataset_path)\n",
    "# adata = sc.read(ADATA_PATH)\n",
    "\n",
    "# adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data module\n",
    "\n",
    "dm = DataModule(\n",
    "    batch_size=config.model[\"hparams\"][\"batch_size\"],\n",
    "    full_eval_during_train=config.train[\"full_eval_during_train\"],\n",
    "    num_workers=config.train[\"num_workers\"],\n",
    "    # num_workers=19,\n",
    "    **config.dataset[\"data_params\"]\n",
    ")\n",
    "\n",
    "# # Check basic stats\n",
    "# print(len(dm.datasets['training']))\n",
    "# print(len(dm.datasets['training_control']))\n",
    "# print(len(dm.datasets['training_treated']))\n",
    "# print(len(dm.datasets['test']))\n",
    "# print(len(dm.datasets['test_control']))\n",
    "# print(len(dm.datasets['test_treated']))\n",
    "# print(len(dm.datasets['ood']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _item in dm.datasets['training']: \n",
    "#     print(_item)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "188\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(dm.datasets['training'].num_covariates)\n",
    "print(dm.datasets['training'].num_drugs)\n",
    "print(dm.datasets['training'].num_knockouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Initialise the model\n",
    "\n",
    "\n",
    "data_train = dm.datasets['training']\n",
    "\n",
    "model = ComPert(\n",
    "    data_train.num_genes,\n",
    "    data_train.num_drugs,\n",
    "    data_train.num_knockouts,\n",
    "    data_train.num_covariates,\n",
    "    config.model.hparams,\n",
    "    config.train,\n",
    "    config.test,\n",
    "    **config.model.additional_params,\n",
    "    drug_embedding_dimension=data_train.drug_embedding_dimension,\n",
    "    knockout_embedding_dimension=data_train.knockout_embedding_dimension,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-hdd/hetzell/miniconda3/envs/chemCPA-test-env-new/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /nfs/staff-hdd/hetzell/miniconda3/envs/chemCPA-test- ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "### Initialise the trainer\n",
    "\n",
    "project_str = f\"{config.model['model_type']}_{config.dataset['dataset_type']}\"\n",
    "wandb_logger = WandbLogger(project=project_str, save_dir=config.model[\"save_dir\"])\n",
    "\n",
    "\n",
    "inference_mode = (not config.train[\"run_eval_disentangle\"]) and (not config.test[\"run_eval_disentangle\"])\n",
    "early_stop_callback = EarlyStopping(\"average_r2_score\", patience=model.hparams.training_params[\"patience\"], mode=\"max\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=config.train[\"num_epochs\"],\n",
    "    max_time=config.train[\"max_time\"],\n",
    "    check_val_every_n_epoch=config.train[\"checkpoint_freq\"],\n",
    "    default_root_dir=config.model[\"save_dir\"],\n",
    "    # profiler=\"advanced\",\n",
    "    callbacks=[early_stop_callback],\n",
    "    inference_mode=inference_mode,\n",
    "    num_sanity_val_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-hdd/hetzell/miniconda3/envs/chemCPA-test-env-new/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:391: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name                   | Type            | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss_autoencoder       | GaussianNLLLoss | 0     \n",
      "1 | encoder                | MLP             | 1.3 M \n",
      "2 | decoder                | MLP             | 1.8 M \n",
      "3 | adversary_drugs        | MLP             | 62.3 K\n",
      "4 | drug_embedding_encoder | MLP             | 644 K \n",
      "5 | loss_adversary_drugs   | CELoss          | 0     \n",
      "6 | dosers                 | MLP             | 12.7 K\n",
      "-----------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.361    Total estimated model params size (MB)\n",
      "/nfs/staff-hdd/hetzell/miniconda3/envs/chemCPA-test-env-new/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266ea63f733a4677ba6220fd3e05f1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |                                                                                                        | 0/? [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "### Test the model\n",
    "\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "\n",
    "#model = ComPert.load_from_checkpoint('train_data/CPA/3gm2eppz/checkpoints/epoch=14-step=10560.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #draw the logfold r2\n",
    "# def draw_logfold_r2(autoencoder, ds_treated, ds_ctrl):\n",
    "#     logfold_score, signs_score = evaluate_logfold_r2(autoencoder, ds_treated, ds_ctrl, return_mean=False)\n",
    "#     df = pd.DataFrame(\n",
    "#         data = {'logfold_score': logfold_score, 'signs_score': signs_score}\n",
    "#     )\n",
    "#     df = pd.melt(df, value_vars=['logfold_score', 'signs_score'], var_name='score_type', value_name='score')\n",
    "#     p = ggplot(df, aes(x='factor(score_type)', y='score', fill='factor(score_type)')) + geom_boxplot() + scale_y_continuous(limits=(-1,1))\n",
    "#     return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "def draw_r2(autoencoder, dataset, genes_control):\n",
    "    mean_score, mean_score_de, var_score, var_score_de = evaluate_r2(autoencoder, dataset, genes_control, return_mean=False)\n",
    "    df = pd.DataFrame(\n",
    "        data = {'mean_score': mean_score, \n",
    "                'mean_score_de': mean_score_de,\n",
    "                'var_score': var_score,\n",
    "                'var_score_de':var_score_de\n",
    "                }\n",
    "    )\n",
    "    df = pd.melt(df, value_vars=['mean_score', 'mean_score_de', 'var_score', 'var_score_de'], \n",
    "                 var_name='score_type', value_name='score')\n",
    "    p = ggplot(df, aes(x='factor(score_type)', y='score', fill='factor(score_type)')) + geom_boxplot()+ scale_y_continuous(limits=(0,1))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #draw the r2 sc\n",
    "# def draw_r2_sc(autoencoder, dataset):\n",
    "#     mean_score, mean_score_de, var_score, var_score_de = evaluate_r2_sc(autoencoder, dataset, return_mean=False)\n",
    "#     df = pd.DataFrame(\n",
    "#         data = {'mean_score': mean_score, \n",
    "#                 'mean_score_de': mean_score_de,\n",
    "#                 'var_score': var_score,\n",
    "#                 'var_score_de':var_score_de\n",
    "#                 }\n",
    "#     )\n",
    "#     df = pd.melt(df, value_vars=['mean_score', 'mean_score_de', 'var_score', 'var_score_de'], \n",
    "#                  var_name='score_type', value_name='score')\n",
    "#     p = ggplot(df, aes(x='factor(score_type)', y='score', fill='factor(score_type)')) + geom_boxplot()+ scale_y_continuous(limits=(0,1))\n",
    "#     return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemCPA-test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
